{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv: kaggle\\input\\neurips-open-polymer-prediction-2025\\train.csv  [OK]\n",
      "test_csv: kaggle\\input\\neurips-open-polymer-prediction-2025\\test.csv  [OK]\n",
      "sample_submission: kaggle\\input\\neurips-open-polymer-prediction-2025\\sample_submission.csv  [OK]\n",
      "tc_data: kaggle\\input\\tc-smiles\\Tc_SMILES.csv  [OK]\n",
      "tg_jcim_data: kaggle\\input\\smiles-extra-data\\JCIM_sup_bigsmiles.csv  [OK]\n",
      "tg_excel_data: kaggle\\input\\smiles-extra-data\\data_tg3.xlsx  [OK]\n",
      "density_data: kaggle\\input\\smiles-extra-data\\data_dnst1.xlsx  [OK]\n",
      "ffv_data: kaggle\\input\\neurips-open-polymer-prediction-2025\\train_supplement\\dataset4.csv  [OK]\n",
      "dataset1: kaggle\\input\\neurips-open-polymer-prediction-2025\\train_supplement\\dataset1.csv  [OK]\n",
      "dataset2: kaggle\\input\\neurips-open-polymer-prediction-2025\\train_supplement\\dataset2.csv  [OK]\n",
      "dataset3: kaggle\\input\\neurips-open-polymer-prediction-2025\\train_supplement\\dataset3.csv  [OK]\n",
      "Train/Test/Sub: (7973, 7) (3, 2) (3, 6)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "from typing import Sequence, Dict, Tuple, Optional, List, Union\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, Lipinski\n",
    "from rdkit.Chem import rdMolDescriptors as rdMD\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "TARGETS = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"] \n",
    "\n",
    "def get_data_paths():\n",
    "    BASE_PATH = Path(os.getenv('NEURIPS_DATA_PATH', 'kaggle/input/neurips-open-polymer-prediction-2025'))\n",
    "    EXTRA_BASE = Path(os.getenv('EXTRA_DATA_BASE', 'kaggle/input/smiles-extra-data'))\n",
    "    TC_BASE = Path(os.getenv('TC_DATA_BASE', 'kaggle/input/tc-smiles'))\n",
    "\n",
    "    paths = {\n",
    "        'train_csv': Path(os.getenv('TRAIN_CSV_PATH', BASE_PATH / 'train.csv')),\n",
    "        'test_csv': Path(os.getenv('TEST_CSV_PATH', BASE_PATH / 'test.csv')),\n",
    "        'sample_submission': Path(os.getenv('SAMPLE_SUBMISSION_PATH', BASE_PATH / 'sample_submission.csv')),\n",
    "\n",
    "        'tc_data': Path(os.getenv('TC_DATA_PATH', TC_BASE / 'Tc_SMILES.csv')),\n",
    "        'tg_jcim_data': Path(os.getenv('TG_JCIM_PATH', EXTRA_BASE / 'JCIM_sup_bigsmiles.csv')),\n",
    "        'tg_excel_data': Path(os.getenv('TG_EXCEL_PATH', EXTRA_BASE / 'data_tg3.xlsx')),\n",
    "        'density_data': Path(os.getenv('DENSITY_PATH', EXTRA_BASE / 'data_dnst1.xlsx')),\n",
    "        'ffv_data': Path(os.getenv('FFV_DATA_PATH', BASE_PATH / 'train_supplement' / 'dataset4.csv')),\n",
    "        'dataset1': Path(os.getenv('DATASET1_PATH', BASE_PATH / 'train_supplement' / 'dataset1.csv')),\n",
    "        'dataset2': Path(os.getenv('DATASET2_PATH', BASE_PATH / 'train_supplement' / 'dataset2.csv')),\n",
    "        'dataset3': Path(os.getenv('DATASET3_PATH', BASE_PATH / 'train_supplement' / 'dataset3.csv')),\n",
    "    }\n",
    "    return paths\n",
    "\n",
    "P = get_data_paths()\n",
    "for k, v in P.items():\n",
    "    print(f\"{k}: {v}  {'[OK]' if v.exists() else '[MISSING]'}\")\n",
    "\n",
    "# 核心官方数据\n",
    "train = pd.read_csv(P['train_csv'])\n",
    "test  = pd.read_csv(P['test_csv'])\n",
    "sub   = pd.read_csv(P['sample_submission'])\n",
    "print(\"Train/Test/Sub:\", train.shape, test.shape, sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_extra_data(df_train: pd.DataFrame, df_extra: pd.DataFrame, target: str, source_name: str = \"extra\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    将外部数据 merge 到 train：\n",
    "    - 如果 train 里有相同 SMILES 且 target 缺失，用 extra 填补\n",
    "    - 如果 extra 里有新 SMILES，追加到 train\n",
    "    - 对同一个 SMILES target 出现多个值时取均值\n",
    "    - 打印出重复 SMILES 的数量 & 差异\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Working on {source_name} (target={target})\")\n",
    "    before = len(df_train)\n",
    "\n",
    "    # 只保留 SMILES + target\n",
    "    df_extra = df_extra[['SMILES', target]].dropna(subset=[target]).copy()\n",
    "\n",
    "    # 聚合外部数据，避免重复\n",
    "    df_extra = df_extra.groupby('SMILES', as_index=False)[target].mean()\n",
    "\n",
    "    # 检查重复\n",
    "    common = set(df_train['SMILES']) & set(df_extra['SMILES'])\n",
    "    if common:\n",
    "        merged_common = pd.merge(\n",
    "            df_train[df_train['SMILES'].isin(common)][['SMILES', target]],\n",
    "            df_extra[df_extra['SMILES'].isin(common)],\n",
    "            on='SMILES',\n",
    "            suffixes=('_train', '_extra')\n",
    "        )\n",
    "        diffs = (merged_common[f\"{target}_train\"] - merged_common[f\"{target}_extra\"]).abs()\n",
    "        print(f\"  重复 SMILES: {len(merged_common)}, 其中 {sum(diffs>1e-6)} 条数值不同 (mean diff={diffs.mean():.3f})\")\n",
    "\n",
    "    # merge\n",
    "    df_train = df_train.merge(df_extra, on='SMILES', how='outer', suffixes=('', '_extra'))\n",
    "\n",
    "    # 填补缺失\n",
    "    mask = df_train[target].isna() & df_train[f\"{target}_extra\"].notna()\n",
    "    df_train.loc[mask, target] = df_train.loc[mask, f\"{target}_extra\"]\n",
    "\n",
    "    # 删除多余列\n",
    "    if f\"{target}_extra\" in df_train.columns:\n",
    "        df_train = df_train.drop(columns=[f\"{target}_extra\"])\n",
    "\n",
    "    after = len(df_train)\n",
    "    print(f\"  [{target}] 增强: {after - before:+d} 条新样本, 填补 {mask.sum()} 条缺失\")\n",
    "\n",
    "    return df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Working on tc_data (target=Tc)\n",
      "  重复 SMILES: 737, 其中 2 条数值不同 (mean diff=0.000)\n",
      "  [Tc] 增强: +130 条新样本, 填补 130 条缺失\n",
      "[INFO] Working on tg_jcim (target=Tg)\n",
      "  重复 SMILES: 7, 其中 0 条数值不同 (mean diff=0.000)\n",
      "  [Tg] 增强: +655 条新样本, 填补 655 条缺失\n",
      "[INFO] Working on tg_excel_K_to_C (target=Tg)\n",
      "  [Tg] 增强: +499 条新样本, 填补 499 条缺失\n",
      "[INFO] Working on dataset3 (target=Tg)\n",
      "  [Tg] 增强: +46 条新样本, 填补 46 条缺失\n",
      "[INFO] Working on density_extra_minus_0p118 (target=Density)\n",
      "  重复 SMILES: 4, 其中 2 条数值不同 (mean diff=0.055)\n",
      "  [Density] 增强: +782 条新样本, 填补 784 条缺失\n",
      "[INFO] Working on dataset1 (target=Tc)\n",
      "  重复 SMILES: 867, 其中 2 条数值不同 (mean diff=0.000)\n",
      "  [Tc] 增强: +0 条新样本, 填补 0 条缺失\n",
      "[INFO] Working on ffv_dataset4 (target=FFV)\n",
      "  重复 SMILES: 37, 其中 0 条数值不同 (mean diff=nan)\n",
      "  [FFV] 增强: +825 条新样本, 填补 862 条缺失\n",
      "train: (10910, 7)\n",
      "targets non-null: {'Tg': 1711, 'FFV': 7892, 'Tc': 867, 'Density': 1397, 'Rg': 614}\n"
     ]
    }
   ],
   "source": [
    "paths = get_data_paths()\n",
    "\n",
    "if os.path.exists(paths['tc_data']):       \n",
    "    train = add_extra_data(\n",
    "        train,\n",
    "        pd.read_csv(paths['tc_data']).rename(columns={'TC_mean':'Tc'}),\n",
    "        target='Tc',\n",
    "        source_name='tc_data'\n",
    "    )\n",
    "\n",
    "if os.path.exists(paths['tg_jcim_data']):  \n",
    "    train = add_extra_data(\n",
    "        train,\n",
    "        pd.read_csv(paths['tg_jcim_data'], usecols=['SMILES','Tg (C)']).rename(columns={'Tg (C)':'Tg'}),\n",
    "        target='Tg',\n",
    "        source_name='tg_jcim'\n",
    "    )\n",
    "\n",
    "if os.path.exists(paths['tg_excel_data']):\n",
    "    train = add_extra_data(\n",
    "        train,\n",
    "        pd.read_excel(paths['tg_excel_data']).rename(columns={'Tg [K]':'Tg'}).assign(Tg=lambda df: df['Tg'] - 273.15),\n",
    "        target='Tg',\n",
    "        source_name='tg_excel_K_to_C'\n",
    "    )\n",
    "\n",
    "if os.path.exists(paths['dataset3']):\n",
    "    train = add_extra_data(\n",
    "        train,\n",
    "        pd.read_csv(paths['dataset3']),  # 已是列名 SMILES, Tg\n",
    "        target='Tg',\n",
    "        source_name='dataset3'\n",
    "    )\n",
    "\n",
    "if os.path.exists(paths['density_data']):\n",
    "    train = add_extra_data(\n",
    "        train,\n",
    "        pd.read_excel(paths['density_data'])\n",
    "          .rename(columns={'density(g/cm3)':'Density'})\n",
    "          .assign(Density=lambda df: pd.to_numeric(df['Density'], errors='coerce') - 0.118),\n",
    "        target='Density',\n",
    "        source_name='density_extra_minus_0p118'\n",
    "    )\n",
    "\n",
    "if os.path.exists(paths['dataset1']):\n",
    "    train = add_extra_data(\n",
    "        train,\n",
    "        pd.read_csv(paths['dataset1']).rename(columns={'TC_mean':'Tc'}),\n",
    "        target='Tc',\n",
    "        source_name='dataset1'\n",
    "    )\n",
    "\n",
    "if os.path.exists(paths['ffv_data']):\n",
    "    train = add_extra_data(\n",
    "        train,\n",
    "        pd.read_csv(paths['ffv_data']).rename(columns={'FFV':'FFV'}),\n",
    "        target='FFV',\n",
    "        source_name='ffv_dataset4'\n",
    "    )\n",
    "\n",
    "print(f\"train: {train.shape}\")\n",
    "print(\"targets non-null:\", {t: int(train[t].notna().sum()) for t in TARGETS if t in train.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in train data:\n",
      "id          2937\n",
      "SMILES         0\n",
      "Tg          9199\n",
      "FFV         3018\n",
      "Tc         10043\n",
      "Density     9513\n",
      "Rg         10296\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values in train data:\")\n",
    "print(train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_smiles_features(smiles: str) -> dict:\n",
    "    \"\"\"提取 SMILES 字符串的字符/化学符号/占位符特征（不依赖 RDKit）\"\"\"\n",
    "    if not isinstance(smiles, str):\n",
    "        smiles = str(smiles)\n",
    "    feats = {}\n",
    "\n",
    "    # 基础统计\n",
    "    feats['smiles_length'] = len(smiles)\n",
    "    feats['capital_letters'] = sum(c.isupper() for c in smiles)\n",
    "    feats['lowercase_letters'] = sum(c.islower() for c in smiles)\n",
    "    feats['digits'] = sum(c.isdigit() for c in smiles)\n",
    "\n",
    "    # 符号统计\n",
    "    feats['parentheses'] = smiles.count('(') + smiles.count(')')\n",
    "    feats['brackets'] = smiles.count('[') + smiles.count(']')\n",
    "    feats['braces'] = smiles.count('{') + smiles.count('}')\n",
    "    feats['equals'] = smiles.count('=')\n",
    "    feats['hashes'] = smiles.count('#')\n",
    "    feats['colons'] = smiles.count(':')\n",
    "    feats['ats'] = smiles.count('@')\n",
    "    feats['slashes'] = smiles.count('/') + smiles.count('\\\\')\n",
    "    feats['plus_minus'] = smiles.count('+') + smiles.count('-')\n",
    "\n",
    "    # 元素计数\n",
    "    feats['C_count'] = smiles.count('C') + smiles.count('c')\n",
    "    feats['O_count'] = smiles.count('O') + smiles.count('o')\n",
    "    feats['N_count'] = smiles.count('N') + smiles.count('n')\n",
    "    feats['S_count'] = smiles.count('S') + smiles.count('s')\n",
    "    feats['P_count'] = smiles.count('P') + smiles.count('p')\n",
    "    feats['F_count'] = smiles.count('F') + smiles.count('f')\n",
    "    feats['Cl_count'] = smiles.count('Cl') + smiles.count('cl')\n",
    "    feats['Br_count'] = smiles.count('Br') + smiles.count('br')\n",
    "    feats['I_count'] = smiles.count('I') + smiles.count('i')\n",
    "\n",
    "    # 结构模式\n",
    "    feats['has_ring'] = int(any(d in smiles for d in '123456789'))\n",
    "    feats['has_double_bond'] = int('=' in smiles)\n",
    "    feats['has_triple_bond'] = int('#' in smiles)\n",
    "    feats['has_aromatic'] = int(any(c in smiles for c in 'cnos'))\n",
    "\n",
    "    # 元素比例\n",
    "    feats['O_to_C_ratio'] = feats['O_count'] / (feats['C_count'] + 1e-5)\n",
    "    feats['N_to_C_ratio'] = feats['N_count'] / (feats['C_count'] + 1e-5)\n",
    "    feats['heteroatom_ratio'] = (\n",
    "        feats['O_count'] + feats['N_count'] + feats['S_count'] + feats['P_count']\n",
    "    ) / (feats['C_count'] + 1e-5)\n",
    "\n",
    "    # 占位符特征\n",
    "    feats['star_count'] = smiles.count('*')\n",
    "    feats['R_placeholder_count'] = len(re.findall(r\"\\[R[0-9']*\\]\", smiles))\n",
    "    feats['any_placeholder'] = int((feats['star_count'] > 0) or (feats['R_placeholder_count'] > 0))\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def add_all_features(df: pd.DataFrame, col: str = 'SMILES') -> pd.DataFrame:\n",
    "    \"\"\"给 DataFrame 添加所有 SMILES 特征\"\"\"\n",
    "    feats = df[col].astype(str).apply(compute_all_smiles_features).apply(pd.Series)\n",
    "    feats.index = df.index\n",
    "    return pd.concat([df, feats], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with features: (10910, 39)\n",
      "Test with features: (3, 34)\n"
     ]
    }
   ],
   "source": [
    "train_basic = add_all_features(train, col='SMILES')\n",
    "test_basic = add_all_features(test,  col='SMILES')\n",
    "\n",
    "print(\"Train with features:\", train_basic.shape)\n",
    "print(\"Test with features:\", test_basic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, exclude_cols=None):\n",
    "    \"\"\"\n",
    "    只对特征列做缺失值填充，排除 id / SMILES / targets 等\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "\n",
    "    # 找出需要处理的数值列\n",
    "    numeric_cols = [\n",
    "        c for c in df.select_dtypes(include=[np.number]).columns\n",
    "        if c not in exclude_cols\n",
    "    ]\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().any():\n",
    "            median_val = df[col].median()\n",
    "            df[col] = df[col].fillna(median_val)\n",
    "\n",
    "    return df\n",
    "EXCLUDE_BASE = ['id', 'SMILES'] + TARGETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in train data:\n",
      "train_basic.shape:(10910, 39)\n",
      "id                      2937\n",
      "SMILES                     0\n",
      "Tg                      9199\n",
      "FFV                     3018\n",
      "Tc                     10043\n",
      "Density                 9513\n",
      "Rg                     10296\n",
      "smiles_length              0\n",
      "capital_letters            0\n",
      "lowercase_letters          0\n",
      "digits                     0\n",
      "parentheses                0\n",
      "brackets                   0\n",
      "braces                     0\n",
      "equals                     0\n",
      "hashes                     0\n",
      "colons                     0\n",
      "ats                        0\n",
      "slashes                    0\n",
      "plus_minus                 0\n",
      "C_count                    0\n",
      "O_count                    0\n",
      "N_count                    0\n",
      "S_count                    0\n",
      "P_count                    0\n",
      "F_count                    0\n",
      "Cl_count                   0\n",
      "Br_count                   0\n",
      "I_count                    0\n",
      "has_ring                   0\n",
      "has_double_bond            0\n",
      "has_triple_bond            0\n",
      "has_aromatic               0\n",
      "O_to_C_ratio               0\n",
      "N_to_C_ratio               0\n",
      "heteroatom_ratio           0\n",
      "star_count                 0\n",
      "R_placeholder_count        0\n",
      "any_placeholder            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_basic = handle_missing_values(train_basic, exclude_cols=EXCLUDE_BASE)\n",
    "test_basic  = handle_missing_values(test_basic, exclude_cols=EXCLUDE_BASE)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in train data:\")\n",
    "print(f'train_basic.shape:{train_basic.shape}')\n",
    "print(train_basic.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem, RDLogger\n",
    "RDLogger.DisableLog('rdApp.error')  # 关掉 RDKit 的报错输出\n",
    "\n",
    "def _mol_from_smiles_robust(smi: str):\n",
    "    \"\"\"更稳的解析：失败时尝试 sanitize=False 再手动 Sanitize。\"\"\"\n",
    "    if not isinstance(smi, str) or not smi.strip():\n",
    "        return None\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is not None:\n",
    "        return mol\n",
    "    mol = Chem.MolFromSmiles(smi, sanitize=False)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    try:\n",
    "        Chem.SanitizeMol(mol)\n",
    "        return mol\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def is_valid_smiles(smi: str) -> bool:\n",
    "    \"\"\"True 表示 RDKit 能成功解析。\"\"\"\n",
    "    return _mol_from_smiles_robust(smi) is not None\n",
    "\n",
    "def drop_invalid_smiles(df, smiles_col: str = \"SMILES\", show_examples: int = 5):\n",
    "    \"\"\"\n",
    "    删除 RDKit 无法解析的 SMILES 行，并打印前后差异。\n",
    "    返回：clean_df（已删除无效行，索引已重置）\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    n_before = len(df)\n",
    "\n",
    "    valid_mask = df[smiles_col].apply(is_valid_smiles)\n",
    "    n_invalid = int((~valid_mask).sum())\n",
    "    pct_invalid = n_invalid / n_before * 100 if n_before > 0 else 0.0\n",
    "\n",
    "    # 打印摘要\n",
    "    print(f\"[SMILES 校验] 总计: {n_before}  | 无效: {n_invalid} ({pct_invalid:.3f}%)\")\n",
    "    if n_invalid > 0:\n",
    "        bad_examples = df.loc[~valid_mask, smiles_col].dropna().astype(str).unique()[:show_examples]\n",
    "        print(\"无效样例（最多展示几条）:\")\n",
    "        for s in bad_examples:\n",
    "            print(\"  -\", s)\n",
    "\n",
    "    # 过滤并返回\n",
    "    clean_df = df.loc[valid_mask].reset_index(drop=True)\n",
    "    print(f\"删除后行数: {len(clean_df)}  | 已删除: {n_invalid}\")\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SMILES 校验] 总计: 10910  | 无效: 6 (0.055%)\n",
      "无效样例（最多展示几条）:\n",
      "  - *C(F)(F)CC(F)([R])C(*)(F)F\n",
      "  - *CN([R'])Cc2cc([R]c1cc(*)c(O)c(CN([R'])C*)c1)cc(*)c2O\n",
      "  - *NC(=O)c4ccc3c(=O)n(c2ccc([R]c1ccc(*)cc1)cc2)c(=O)c3c4\n",
      "  - *OC2OC(CO[R])C(OC1OC(CO[R])C(*)C(O[R])C1O[R])C(O[R])C2O[R]\n",
      "  - *O[Si](*)([R])[R]\n",
      "删除后行数: 10904  | 已删除: 6\n",
      "[SMILES 校验] 总计: 3  | 无效: 0 (0.000%)\n",
      "删除后行数: 3  | 已删除: 0\n"
     ]
    }
   ],
   "source": [
    "# 仅对训练集删除无效 SMILES（测试集不要删）\n",
    "train_clean = drop_invalid_smiles(train_basic, smiles_col=\"SMILES\")\n",
    "\n",
    "# 如果想看看测试集的无效比例，但不删除：\n",
    "test_clean = drop_invalid_smiles(test_basic, smiles_col=\"SMILES\", show_examples=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdkit_descriptors_from_list(smiles_series: pd.Series, desc_names: list, on_fail=np.nan) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    只支持 rdkit.Chem.Descriptors.descList 中的名字。\n",
    "    - 未知名称会直接报错（帮助你发现拼写问题）\n",
    "    - 解析失败的 SMILES 行填 on_fail（默认 NaN）\n",
    "    返回：与输入索引对齐的 DataFrame\n",
    "    \"\"\"\n",
    "    # descList 是 (name, func) 的列表\n",
    "    avail = dict(Descriptors.descList)\n",
    "    bad = [n for n in desc_names if n not in avail]\n",
    "    if bad:\n",
    "        raise ValueError(f\"Unknown descriptor names (not in Descriptors.descList): {bad}\")\n",
    "\n",
    "    out_rows = []\n",
    "    for smi in smiles_series.astype(str).tolist():\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol is None:\n",
    "            out_rows.append({n: on_fail for n in desc_names})\n",
    "            continue\n",
    "        row = {}\n",
    "        for n in desc_names:\n",
    "            try:\n",
    "                row[n] = float(avail[n](mol))\n",
    "            except Exception:\n",
    "                row[n] = on_fail\n",
    "        out_rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(out_rows, index=smiles_series.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 1) 计算 RDKit 全量官方描述符（不含手工字段） =====\n",
    "def compute_all_rdkit_descriptors(smiles_series: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    计算所有RDKit描述符，显示进度条和统计信息\n",
    "    \"\"\"\n",
    "    names, funcs = zip(*Descriptors.descList)\n",
    "    rows = []\n",
    "    invalid_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # 使用tqdm\n",
    "    pbar = tqdm(smiles_series.astype(str), \n",
    "                total=len(smiles_series),\n",
    "                desc=\"计算RDKit描述符\",\n",
    "                unit=\"mol\")\n",
    "    \n",
    "    for s in pbar:\n",
    "        mol = Chem.MolFromSmiles(s)\n",
    "        if mol is None:\n",
    "            rows.append([np.nan] * len(funcs))\n",
    "            invalid_count += 1\n",
    "            pbar.set_postfix({'无效SMILES': invalid_count, '计算错误': error_count})\n",
    "            continue\n",
    "        \n",
    "        vals = []\n",
    "        for f in funcs:\n",
    "            try:\n",
    "                vals.append(float(f(mol)))\n",
    "            except Exception as e:\n",
    "                vals.append(np.nan)\n",
    "                error_count += 1\n",
    "        \n",
    "        rows.append(vals)\n",
    "        pbar.set_postfix({'无效SMILES': invalid_count, '计算错误': error_count})\n",
    "    \n",
    "    # 打印统计信息\n",
    "    print(f\"\\n完成！统计信息:\")\n",
    "    print(f\"总分子数: {len(smiles_series)}\")\n",
    "    print(f\"无效SMILES: {invalid_count}\")\n",
    "    print(f\"描述符计算错误次数: {error_count}\")\n",
    "    \n",
    "    return pd.DataFrame(rows, columns=list(names), index=smiles_series.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_feature_list(feature_spec: Union[Sequence[str], Dict[str, Sequence[str]]], target: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    兼容两种写法：\n",
    "    - 直接给一个 list/tuple 的描述符名\n",
    "    - 给 {target: [names]} 的字典\n",
    "    \"\"\"\n",
    "    if isinstance(feature_spec, dict):\n",
    "        if target not in feature_spec:\n",
    "            raise KeyError(f\"feature_spec 没有键 {target}\")\n",
    "        names = feature_spec[target]\n",
    "    else:\n",
    "        names = feature_spec\n",
    "    # 去重但保序\n",
    "    return list(dict.fromkeys(map(str, names)))\n",
    "\n",
    "def _align_columns(X_tr: pd.DataFrame, X_te: pd.DataFrame, fill_value=np.nan) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    cols = list(dict.fromkeys(list(X_tr.columns) + list(X_te.columns)))\n",
    "    return (X_tr.reindex(columns=cols, fill_value=fill_value),\n",
    "            X_te.reindex(columns=cols, fill_value=fill_value))\n",
    "\n",
    "def make_dataset_for_target(\n",
    "    *,\n",
    "    train_df: pd.DataFrame,      # 含 target 列（例如 'Tg'）\n",
    "    test_df: pd.DataFrame,       # 只用来对齐行数/方便将来扩展\n",
    "    desc_train: pd.DataFrame,    # compute_all_rdkit_descriptors(train_smiles)\n",
    "    desc_test: pd.DataFrame,     # compute_all_rdkit_descriptors(test_smiles)\n",
    "    target: str,                 # 'Tg' / 'FFV' / 'Tc' / 'Density' / 'Rg'\n",
    "    feature_spec: Union[Sequence[str], Dict[str, Sequence[str]]],\n",
    "    prefix: Optional[str] = None,\n",
    "    drop_missing: bool = True,\n",
    "    verbose: bool = True,\n",
    ") -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    用一行调用：按给定描述符名，直接产出 X_train, y_train, X_test。\n",
    "    - feature_spec: 可是 ['MolWt','TPSA',...] 或 {'Tg': [...]} 两种形式\n",
    "    - 会自动对齐 train/test 列，缺的列以 NaN 填；树模型可直接吃 NaN\n",
    "    - 返回 info 内含 kept/missing/形状等信息\n",
    "    \"\"\"\n",
    "    names = _as_feature_list(feature_spec, target)\n",
    "    exist_tr = [c for c in names if c in desc_train.columns]\n",
    "    exist_te = [c for c in names if c in desc_test.columns]\n",
    "    missing  = [c for c in names if c not in desc_train.columns or c not in desc_test.columns]\n",
    "\n",
    "    if missing and not drop_missing:\n",
    "        raise KeyError(f\"{target}: 这些描述符列缺失：{missing}\")\n",
    "\n",
    "    Xtr_raw = desc_train[exist_tr].copy()\n",
    "    Xte_raw = desc_test[exist_te].copy()\n",
    "    Xtr, Xte = _align_columns(Xtr_raw, Xte_raw, fill_value=np.nan)\n",
    "\n",
    "    # 选出有标签的行\n",
    "    mask = train_df[target].notna()\n",
    "    y = train_df.loc[mask, target].astype(float).copy()\n",
    "    Xtr = Xtr.loc[mask].copy()\n",
    "\n",
    "    # 可选加前缀，避免后续多目标拼表时冲突\n",
    "    if prefix:\n",
    "        Xtr.columns = [f\"{prefix}{c}\" for c in Xtr.columns]\n",
    "        Xte.columns = [f\"{prefix}{c}\" for c in Xte.columns]\n",
    "\n",
    "    # 统一 dtype（对树模型友好）\n",
    "    Xtr = Xtr.astype(np.float32)\n",
    "    Xte = Xte.astype(np.float32)\n",
    "\n",
    "    info = dict(\n",
    "        target=target,\n",
    "        requested=len(names),\n",
    "        kept=len(Xtr.columns),\n",
    "        missing=missing,\n",
    "        Xtr_shape=Xtr.shape,\n",
    "        Xte_shape=Xte.shape,\n",
    "        y_len=len(y),\n",
    "    )\n",
    "    if verbose:\n",
    "        print(f\"[{target}] requested={len(names)}, kept={len(Xtr.columns)}, missing={len(missing)}\")\n",
    "        if missing:\n",
    "            print(f\" missing: {missing[:12]}{' ...' if len(missing)>12 else ''}\")\n",
    "        print(f\" Xtr={Xtr.shape}, Xte={Xte.shape}, y={len(y)}\")\n",
    "\n",
    "    return Xtr, y, Xte, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算RDKit描述符:   0%|          | 0/10904 [00:00<?, ?mol/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算RDKit描述符: 100%|██████████| 10904/10904 [02:04<00:00, 87.40mol/s, 无效SMILES=0, 计算错误=0] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "完成！统计信息:\n",
      "总分子数: 10904\n",
      "无效SMILES: 0\n",
      "描述符计算错误次数: 0\n"
     ]
    }
   ],
   "source": [
    "# 先一次性算好 RDKit 全量描述符\n",
    "desc_train = compute_all_rdkit_descriptors(train_clean['SMILES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算RDKit描述符: 100%|██████████| 3/3 [00:00<00:00, 82.09mol/s, 无效SMILES=0, 计算错误=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "完成！统计信息:\n",
      "总分子数: 3\n",
      "无效SMILES: 0\n",
      "描述符计算错误次数: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "desc_test = compute_all_rdkit_descriptors(test_clean['SMILES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有缺失值的列数: 12\n",
      "有缺失值的列:\n",
      "MaxPartialCharge        9626\n",
      "MinPartialCharge        9626\n",
      "MaxAbsPartialCharge     9626\n",
      "MinAbsPartialCharge     9626\n",
      "BCUT2D_MWHI            10094\n",
      "BCUT2D_MWLOW           10094\n",
      "BCUT2D_CHGHI           10094\n",
      "BCUT2D_CHGLO           10094\n",
      "BCUT2D_LOGPHI          10094\n",
      "BCUT2D_LOGPLOW         10094\n",
      "BCUT2D_MRHI            10094\n",
      "BCUT2D_MRLOW           10094\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 只查看有缺失值的列\n",
    "missing_stats = desc_train.isna().sum()\n",
    "columns_with_missing = missing_stats[missing_stats > 0]\n",
    "\n",
    "print(f\"有缺失值的列数: {len(columns_with_missing)}\")\n",
    "print(\"有缺失值的列:\")\n",
    "print(columns_with_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有缺失值的列数: 12\n",
      "有缺失值的列:\n",
      "MaxPartialCharge       3\n",
      "MinPartialCharge       3\n",
      "MaxAbsPartialCharge    3\n",
      "MinAbsPartialCharge    3\n",
      "BCUT2D_MWHI            3\n",
      "BCUT2D_MWLOW           3\n",
      "BCUT2D_CHGHI           3\n",
      "BCUT2D_CHGLO           3\n",
      "BCUT2D_LOGPHI          3\n",
      "BCUT2D_LOGPLOW         3\n",
      "BCUT2D_MRHI            3\n",
      "BCUT2D_MRLOW           3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 只查看有缺失值的列\n",
    "missing_stats = desc_test.isna().sum()\n",
    "columns_with_missing = missing_stats[missing_stats > 0]\n",
    "\n",
    "print(f\"有缺失值的列数: {len(columns_with_missing)}\")\n",
    "print(\"有缺失值的列:\")\n",
    "print(columns_with_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_Tg = ['MolWt','MolLogP','TPSA','NumRotatableBonds','NumRings','FractionCSP3','NumHAcceptors','NumHDonors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tg] requested=8, kept=7, missing=1\n",
      " missing: ['NumRings']\n",
      " Xtr=(1711, 7), Xte=(3, 7), y=1711\n"
     ]
    }
   ],
   "source": [
    "X_tr, y_tr, X_te, info = make_dataset_for_target(\n",
    "    train_df=train_clean,\n",
    "    test_df=test_clean,\n",
    "    desc_train=desc_train,\n",
    "    desc_test=desc_test,\n",
    "    target='Tg',\n",
    "    feature_spec=feature_Tg,   # 或 {'Tg': feature_Tg}\n",
    "    prefix=None,               # 想要列名带前缀就填比如 'feat_Tg__'\n",
    "    drop_missing=True,\n",
    "    verbose=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurips",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
