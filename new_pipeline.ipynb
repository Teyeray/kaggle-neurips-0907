{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv: kaggle/input/neurips-open-polymer-prediction-2025/train.csv  [OK]\n",
      "test_csv: kaggle/input/neurips-open-polymer-prediction-2025/test.csv  [OK]\n",
      "sample_submission: kaggle/input/neurips-open-polymer-prediction-2025/sample_submission.csv  [OK]\n",
      "tc_data: kaggle/input/tc-smiles/Tc_SMILES.csv  [OK]\n",
      "tg_jcim_data: kaggle/input/smiles-extra-data/JCIM_sup_bigsmiles.csv  [OK]\n",
      "tg_excel_data: kaggle/input/smiles-extra-data/data_tg3.xlsx  [OK]\n",
      "density_data: kaggle/input/smiles-extra-data/data_dnst1.xlsx  [OK]\n",
      "ffv_data: kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset4.csv  [OK]\n",
      "dataset1: kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset1.csv  [OK]\n",
      "dataset2: kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset2.csv  [OK]\n",
      "dataset3: kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset3.csv  [OK]\n",
      "Train/Test/Sub: (7973, 7) (3, 2) (3, 6)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, Lipinski\n",
    "from rdkit.Chem import rdMolDescriptors as rdMD\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "TARGETS = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"] \n",
    "\n",
    "def get_data_paths():\n",
    "    BASE_PATH = Path(os.getenv('NEURIPS_DATA_PATH', 'kaggle/input/neurips-open-polymer-prediction-2025'))\n",
    "    EXTRA_BASE = Path(os.getenv('EXTRA_DATA_BASE', 'kaggle/input/smiles-extra-data'))\n",
    "    TC_BASE = Path(os.getenv('TC_DATA_BASE', 'kaggle/input/tc-smiles'))\n",
    "\n",
    "    paths = {\n",
    "        'train_csv': Path(os.getenv('TRAIN_CSV_PATH', BASE_PATH / 'train.csv')),\n",
    "        'test_csv': Path(os.getenv('TEST_CSV_PATH', BASE_PATH / 'test.csv')),\n",
    "        'sample_submission': Path(os.getenv('SAMPLE_SUBMISSION_PATH', BASE_PATH / 'sample_submission.csv')),\n",
    "\n",
    "        'tc_data': Path(os.getenv('TC_DATA_PATH', TC_BASE / 'Tc_SMILES.csv')),\n",
    "        'tg_jcim_data': Path(os.getenv('TG_JCIM_PATH', EXTRA_BASE / 'JCIM_sup_bigsmiles.csv')),\n",
    "        'tg_excel_data': Path(os.getenv('TG_EXCEL_PATH', EXTRA_BASE / 'data_tg3.xlsx')),\n",
    "        'density_data': Path(os.getenv('DENSITY_PATH', EXTRA_BASE / 'data_dnst1.xlsx')),\n",
    "        'ffv_data': Path(os.getenv('FFV_DATA_PATH', BASE_PATH / 'train_supplement' / 'dataset4.csv')),\n",
    "        'dataset1': Path(os.getenv('DATASET1_PATH', BASE_PATH / 'train_supplement' / 'dataset1.csv')),\n",
    "        'dataset2': Path(os.getenv('DATASET2_PATH', BASE_PATH / 'train_supplement' / 'dataset2.csv')),\n",
    "        'dataset3': Path(os.getenv('DATASET3_PATH', BASE_PATH / 'train_supplement' / 'dataset3.csv')),\n",
    "    }\n",
    "    return paths\n",
    "\n",
    "P = get_data_paths()\n",
    "for k, v in P.items():\n",
    "    print(f\"{k}: {v}  {'[OK]' if v.exists() else '[MISSING]'}\")\n",
    "\n",
    "# 核心官方数据\n",
    "train = pd.read_csv(P['train_csv'])\n",
    "test  = pd.read_csv(P['test_csv'])\n",
    "sub   = pd.read_csv(P['sample_submission'])\n",
    "print(\"Train/Test/Sub:\", train.shape, test.shape, sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_extra_data(df_train: pd.DataFrame, df_extra: pd.DataFrame, target: str, source_name: str = \"extra\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    将外部数据 merge 到 train：\n",
    "    - 如果 train 里有相同 SMILES 且 target 缺失，用 extra 填补\n",
    "    - 如果 extra 里有新 SMILES，追加到 train\n",
    "    - 对同一个 SMILES target 出现多个值时取均值\n",
    "    - 打印出重复 SMILES 的数量 & 差异\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Working on {source_name} (target={target})\")\n",
    "    before = len(df_train)\n",
    "\n",
    "    # 只保留 SMILES + target\n",
    "    df_extra = df_extra[['SMILES', target]].dropna(subset=[target]).copy()\n",
    "\n",
    "    # 聚合外部数据，避免重复\n",
    "    df_extra = df_extra.groupby('SMILES', as_index=False)[target].mean()\n",
    "\n",
    "    # 检查重复\n",
    "    common = set(df_train['SMILES']) & set(df_extra['SMILES'])\n",
    "    if common:\n",
    "        merged_common = pd.merge(\n",
    "            df_train[df_train['SMILES'].isin(common)][['SMILES', target]],\n",
    "            df_extra[df_extra['SMILES'].isin(common)],\n",
    "            on='SMILES',\n",
    "            suffixes=('_train', '_extra')\n",
    "        )\n",
    "        diffs = (merged_common[f\"{target}_train\"] - merged_common[f\"{target}_extra\"]).abs()\n",
    "        print(f\"  重复 SMILES: {len(merged_common)}, 其中 {sum(diffs>1e-6)} 条数值不同 (mean diff={diffs.mean():.3f})\")\n",
    "\n",
    "    # merge\n",
    "    df_train = df_train.merge(df_extra, on='SMILES', how='outer', suffixes=('', '_extra'))\n",
    "\n",
    "    # 填补缺失\n",
    "    mask = df_train[target].isna() & df_train[f\"{target}_extra\"].notna()\n",
    "    df_train.loc[mask, target] = df_train.loc[mask, f\"{target}_extra\"]\n",
    "\n",
    "    # 删除多余列\n",
    "    if f\"{target}_extra\" in df_train.columns:\n",
    "        df_train = df_train.drop(columns=[f\"{target}_extra\"])\n",
    "\n",
    "    after = len(df_train)\n",
    "    print(f\"  [{target}] 增强: {after - before:+d} 条新样本, 填补 {mask.sum()} 条缺失\")\n",
    "\n",
    "    return df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Working on tc_data (target=Tc)\n",
      "  重复 SMILES: 737, 其中 2 条数值不同 (mean diff=0.000)\n",
      "  [Tc] 增强: +130 条新样本, 填补 130 条缺失\n",
      "[INFO] Working on tg_jcim (target=Tg)\n",
      "  重复 SMILES: 7, 其中 0 条数值不同 (mean diff=0.000)\n",
      "  [Tg] 增强: +655 条新样本, 填补 655 条缺失\n",
      "[INFO] Working on tg_excel_K_to_C (target=Tg)\n",
      "  [Tg] 增强: +499 条新样本, 填补 499 条缺失\n",
      "[INFO] Working on dataset3 (target=Tg)\n",
      "  [Tg] 增强: +46 条新样本, 填补 46 条缺失\n",
      "[INFO] Working on density_extra_minus_0p118 (target=Density)\n",
      "  重复 SMILES: 4, 其中 2 条数值不同 (mean diff=0.055)\n",
      "  [Density] 增强: +782 条新样本, 填补 784 条缺失\n",
      "[INFO] Working on dataset1 (target=Tc)\n",
      "  重复 SMILES: 867, 其中 2 条数值不同 (mean diff=0.000)\n",
      "  [Tc] 增强: +0 条新样本, 填补 0 条缺失\n",
      "[INFO] Working on ffv_dataset4 (target=FFV)\n",
      "  重复 SMILES: 37, 其中 0 条数值不同 (mean diff=nan)\n",
      "  [FFV] 增强: +825 条新样本, 填补 862 条缺失\n",
      "train: (10910, 7)\n",
      "targets non-null: {'Tg': 1711, 'FFV': 7892, 'Tc': 867, 'Density': 1397, 'Rg': 614}\n"
     ]
    }
   ],
   "source": [
    "paths = get_data_paths()\n",
    "\n",
    "if os.path.exists(paths['tc_data']):       \n",
    "    train = add_extra_data(\n",
    "        train,\n",
    "        pd.read_csv(paths['tc_data']).rename(columns={'TC_mean':'Tc'}),\n",
    "        target='Tc',\n",
    "        source_name='tc_data'\n",
    "    )\n",
    "\n",
    "if os.path.exists(paths['tg_jcim_data']):  \n",
    "    train = add_extra_data(\n",
    "        train,\n",
    "        pd.read_csv(paths['tg_jcim_data'], usecols=['SMILES','Tg (C)']).rename(columns={'Tg (C)':'Tg'}),\n",
    "        target='Tg',\n",
    "        source_name='tg_jcim'\n",
    "    )\n",
    "\n",
    "if os.path.exists(paths['tg_excel_data']):\n",
    "    train = add_extra_data(\n",
    "        train,\n",
    "        pd.read_excel(paths['tg_excel_data']).rename(columns={'Tg [K]':'Tg'}).assign(Tg=lambda df: df['Tg'] - 273.15),\n",
    "        target='Tg',\n",
    "        source_name='tg_excel_K_to_C'\n",
    "    )\n",
    "\n",
    "if os.path.exists(paths['dataset3']):\n",
    "    train = add_extra_data(\n",
    "        train,\n",
    "        pd.read_csv(paths['dataset3']),  # 已是列名 SMILES, Tg\n",
    "        target='Tg',\n",
    "        source_name='dataset3'\n",
    "    )\n",
    "\n",
    "if os.path.exists(paths['density_data']):\n",
    "    train = add_extra_data(\n",
    "        train,\n",
    "        pd.read_excel(paths['density_data'])\n",
    "          .rename(columns={'density(g/cm3)':'Density'})\n",
    "          .assign(Density=lambda df: pd.to_numeric(df['Density'], errors='coerce') - 0.118),\n",
    "        target='Density',\n",
    "        source_name='density_extra_minus_0p118'\n",
    "    )\n",
    "\n",
    "if os.path.exists(paths['dataset1']):\n",
    "    train = add_extra_data(\n",
    "        train,\n",
    "        pd.read_csv(paths['dataset1']).rename(columns={'TC_mean':'Tc'}),\n",
    "        target='Tc',\n",
    "        source_name='dataset1'\n",
    "    )\n",
    "\n",
    "if os.path.exists(paths['ffv_data']):\n",
    "    train = add_extra_data(\n",
    "        train,\n",
    "        pd.read_csv(paths['ffv_data']).rename(columns={'FFV':'FFV'}),\n",
    "        target='FFV',\n",
    "        source_name='ffv_dataset4'\n",
    "    )\n",
    "\n",
    "print(f\"train: {train.shape}\")\n",
    "print(\"targets non-null:\", {t: int(train[t].notna().sum()) for t in TARGETS if t in train.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in train data:\n",
      "id          2937\n",
      "SMILES         0\n",
      "Tg          9199\n",
      "FFV         3018\n",
      "Tc         10043\n",
      "Density     9513\n",
      "Rg         10296\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values in train data:\")\n",
    "print(train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_smiles_features(smiles: str) -> dict:\n",
    "    \"\"\"提取 SMILES 字符串的字符/化学符号/占位符特征（不依赖 RDKit）\"\"\"\n",
    "    if not isinstance(smiles, str):\n",
    "        smiles = str(smiles)\n",
    "    feats = {}\n",
    "\n",
    "    # 基础统计\n",
    "    feats['smiles_length'] = len(smiles)\n",
    "    feats['capital_letters'] = sum(c.isupper() for c in smiles)\n",
    "    feats['lowercase_letters'] = sum(c.islower() for c in smiles)\n",
    "    feats['digits'] = sum(c.isdigit() for c in smiles)\n",
    "\n",
    "    # 符号统计\n",
    "    feats['parentheses'] = smiles.count('(') + smiles.count(')')\n",
    "    feats['brackets'] = smiles.count('[') + smiles.count(']')\n",
    "    feats['braces'] = smiles.count('{') + smiles.count('}')\n",
    "    feats['equals'] = smiles.count('=')\n",
    "    feats['hashes'] = smiles.count('#')\n",
    "    feats['colons'] = smiles.count(':')\n",
    "    feats['ats'] = smiles.count('@')\n",
    "    feats['slashes'] = smiles.count('/') + smiles.count('\\\\')\n",
    "    feats['plus_minus'] = smiles.count('+') + smiles.count('-')\n",
    "\n",
    "    # 元素计数\n",
    "    feats['C_count'] = smiles.count('C') + smiles.count('c')\n",
    "    feats['O_count'] = smiles.count('O') + smiles.count('o')\n",
    "    feats['N_count'] = smiles.count('N') + smiles.count('n')\n",
    "    feats['S_count'] = smiles.count('S') + smiles.count('s')\n",
    "    feats['P_count'] = smiles.count('P') + smiles.count('p')\n",
    "    feats['F_count'] = smiles.count('F') + smiles.count('f')\n",
    "    feats['Cl_count'] = smiles.count('Cl') + smiles.count('cl')\n",
    "    feats['Br_count'] = smiles.count('Br') + smiles.count('br')\n",
    "    feats['I_count'] = smiles.count('I') + smiles.count('i')\n",
    "\n",
    "    # 结构模式\n",
    "    feats['has_ring'] = int(any(d in smiles for d in '123456789'))\n",
    "    feats['has_double_bond'] = int('=' in smiles)\n",
    "    feats['has_triple_bond'] = int('#' in smiles)\n",
    "    feats['has_aromatic'] = int(any(c in smiles for c in 'cnos'))\n",
    "\n",
    "    # 元素比例\n",
    "    feats['O_to_C_ratio'] = feats['O_count'] / (feats['C_count'] + 1e-5)\n",
    "    feats['N_to_C_ratio'] = feats['N_count'] / (feats['C_count'] + 1e-5)\n",
    "    feats['heteroatom_ratio'] = (\n",
    "        feats['O_count'] + feats['N_count'] + feats['S_count'] + feats['P_count']\n",
    "    ) / (feats['C_count'] + 1e-5)\n",
    "\n",
    "    # 占位符特征\n",
    "    feats['star_count'] = smiles.count('*')\n",
    "    feats['R_placeholder_count'] = len(re.findall(r\"\\[R[0-9']*\\]\", smiles))\n",
    "    feats['any_placeholder'] = int((feats['star_count'] > 0) or (feats['R_placeholder_count'] > 0))\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def add_all_features(df: pd.DataFrame, col: str = 'SMILES') -> pd.DataFrame:\n",
    "    \"\"\"给 DataFrame 添加所有 SMILES 特征\"\"\"\n",
    "    feats = df[col].astype(str).apply(compute_all_smiles_features).apply(pd.Series)\n",
    "    feats.index = df.index\n",
    "    return pd.concat([df, feats], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with features: (10910, 39)\n",
      "Test with features: (3, 34)\n"
     ]
    }
   ],
   "source": [
    "train_with_feats = add_all_features(train, col='SMILES')\n",
    "test_with_feats  = add_all_features(test,  col='SMILES')\n",
    "\n",
    "print(\"Train with features:\", train_with_feats.shape)\n",
    "print(\"Test with features:\", test_with_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id                                 SMILES          Tg  FFV     Tc  \\\n",
      "0          NaN          */C(=C(/*)\\c1ccccc1)/c1ccccc1  206.569886  NaN    NaN   \n",
      "1  218059466.0            */C(=C(/*)c1ccccc1)c1ccccc1  206.569886  NaN    NaN   \n",
      "2          NaN  */C(=C(\\c1ccccc1)c1ccc(*)cc1)c1ccccc1         NaN  NaN  0.338   \n",
      "\n",
      "   Density  Rg  smiles_length  capital_letters  lowercase_letters  ...  \\\n",
      "0      NaN NaN           29.0              2.0               12.0  ...   \n",
      "1      NaN NaN           27.0              2.0               12.0  ...   \n",
      "2      NaN NaN           37.0              2.0               18.0  ...   \n",
      "\n",
      "   has_ring  has_double_bond  has_triple_bond  has_aromatic  O_to_C_ratio  \\\n",
      "0       1.0              1.0              0.0           1.0           0.0   \n",
      "1       1.0              1.0              0.0           1.0           0.0   \n",
      "2       1.0              1.0              0.0           1.0           0.0   \n",
      "\n",
      "   N_to_C_ratio  heteroatom_ratio  star_count  R_placeholder_count  \\\n",
      "0           0.0               0.0         2.0                  0.0   \n",
      "1           0.0               0.0         2.0                  0.0   \n",
      "2           0.0               0.0         2.0                  0.0   \n",
      "\n",
      "   any_placeholder  \n",
      "0              1.0  \n",
      "1              1.0  \n",
      "2              1.0  \n",
      "\n",
      "[3 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_with_feats.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, exclude_cols=None):\n",
    "    \"\"\"\n",
    "    只对特征列做缺失值填充，排除 id / SMILES / targets 等\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "\n",
    "    # 找出需要处理的数值列\n",
    "    numeric_cols = [\n",
    "        c for c in df.select_dtypes(include=[np.number]).columns\n",
    "        if c not in exclude_cols\n",
    "    ]\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().any():\n",
    "            median_val = df[col].median()\n",
    "            df[col] = df[col].fillna(median_val)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in train data:\n",
      "train_df.shape:(10910, 38)\n",
      "SMILES                     0\n",
      "Tg                      9199\n",
      "FFV                     3018\n",
      "Tc                     10043\n",
      "Density                 9513\n",
      "Rg                     10296\n",
      "smiles_length              0\n",
      "capital_letters            0\n",
      "lowercase_letters          0\n",
      "digits                     0\n",
      "parentheses                0\n",
      "brackets                   0\n",
      "braces                     0\n",
      "equals                     0\n",
      "hashes                     0\n",
      "colons                     0\n",
      "ats                        0\n",
      "slashes                    0\n",
      "plus_minus                 0\n",
      "C_count                    0\n",
      "O_count                    0\n",
      "N_count                    0\n",
      "S_count                    0\n",
      "P_count                    0\n",
      "F_count                    0\n",
      "Cl_count                   0\n",
      "Br_count                   0\n",
      "I_count                    0\n",
      "has_ring                   0\n",
      "has_double_bond            0\n",
      "has_triple_bond            0\n",
      "has_aromatic               0\n",
      "O_to_C_ratio               0\n",
      "N_to_C_ratio               0\n",
      "heteroatom_ratio           0\n",
      "star_count                 0\n",
      "R_placeholder_count        0\n",
      "any_placeholder            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "EXCLUDE_BASE = ['id', 'SMILES'] + TARGETS\n",
    "\n",
    "train_df = handle_missing_values(train_with_feats, exclude_cols=EXCLUDE_BASE)\n",
    "test_df  = handle_missing_values(test_with_feats, exclude_cols=EXCLUDE_BASE)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in train data:\")\n",
    "train_df = train_df.drop(columns=['id'])\n",
    "print(f'train_df.shape:{train_df.shape}')\n",
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem, RDLogger\n",
    "RDLogger.DisableLog('rdApp.error')  # 关掉 RDKit 的报错输出\n",
    "\n",
    "def _mol_from_smiles_robust(smi: str):\n",
    "    \"\"\"更稳的解析：失败时尝试 sanitize=False 再手动 Sanitize。\"\"\"\n",
    "    if not isinstance(smi, str) or not smi.strip():\n",
    "        return None\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is not None:\n",
    "        return mol\n",
    "    mol = Chem.MolFromSmiles(smi, sanitize=False)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    try:\n",
    "        Chem.SanitizeMol(mol)\n",
    "        return mol\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def is_valid_smiles(smi: str) -> bool:\n",
    "    \"\"\"True 表示 RDKit 能成功解析。\"\"\"\n",
    "    return _mol_from_smiles_robust(smi) is not None\n",
    "\n",
    "def drop_invalid_smiles(df, smiles_col: str = \"SMILES\", show_examples: int = 5):\n",
    "    \"\"\"\n",
    "    删除 RDKit 无法解析的 SMILES 行，并打印前后差异。\n",
    "    返回：clean_df（已删除无效行，索引已重置）\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    n_before = len(df)\n",
    "\n",
    "    valid_mask = df[smiles_col].apply(is_valid_smiles)\n",
    "    n_invalid = int((~valid_mask).sum())\n",
    "    pct_invalid = n_invalid / n_before * 100 if n_before > 0 else 0.0\n",
    "\n",
    "    # 打印摘要\n",
    "    print(f\"[SMILES 校验] 总计: {n_before}  | 无效: {n_invalid} ({pct_invalid:.3f}%)\")\n",
    "    if n_invalid > 0:\n",
    "        bad_examples = df.loc[~valid_mask, smiles_col].dropna().astype(str).unique()[:show_examples]\n",
    "        print(\"无效样例（最多展示几条）:\")\n",
    "        for s in bad_examples:\n",
    "            print(\"  -\", s)\n",
    "\n",
    "    # 过滤并返回\n",
    "    clean_df = df.loc[valid_mask].reset_index(drop=True)\n",
    "    print(f\"删除后行数: {len(clean_df)}  | 已删除: {n_invalid}\")\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SMILES 校验] 总计: 10910  | 无效: 6 (0.055%)\n",
      "无效样例（最多展示几条）:\n",
      "  - *C(F)(F)CC(F)([R])C(*)(F)F\n",
      "  - *CN([R'])Cc2cc([R]c1cc(*)c(O)c(CN([R'])C*)c1)cc(*)c2O\n",
      "  - *NC(=O)c4ccc3c(=O)n(c2ccc([R]c1ccc(*)cc1)cc2)c(=O)c3c4\n",
      "  - *OC2OC(CO[R])C(OC1OC(CO[R])C(*)C(O[R])C1O[R])C(O[R])C2O[R]\n",
      "  - *O[Si](*)([R])[R]\n",
      "删除后行数: 10904  | 已删除: 6\n",
      "[SMILES 校验] 总计: 3  | 无效: 0 (0.000%)\n",
      "删除后行数: 3  | 已删除: 0\n"
     ]
    }
   ],
   "source": [
    "# 仅对训练集删除无效 SMILES（测试集不要删）\n",
    "train_clean = drop_invalid_smiles(train_df, smiles_col=\"SMILES\")\n",
    "\n",
    "# 如果想看看测试集的无效比例，但不删除：\n",
    "_ = drop_invalid_smiles(test, smiles_col=\"SMILES\", show_examples=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdkit_descriptors_from_list(smiles_series: pd.Series, desc_names: list, on_fail=np.nan) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    只支持 rdkit.Chem.Descriptors.descList 中的名字。\n",
    "    - 未知名称会直接报错（帮助你发现拼写问题）\n",
    "    - 解析失败的 SMILES 行填 on_fail（默认 NaN）\n",
    "    返回：与输入索引对齐的 DataFrame\n",
    "    \"\"\"\n",
    "    # descList 是 (name, func) 的列表\n",
    "    avail = dict(Descriptors.descList)\n",
    "    bad = [n for n in desc_names if n not in avail]\n",
    "    if bad:\n",
    "        raise ValueError(f\"Unknown descriptor names (not in Descriptors.descList): {bad}\")\n",
    "\n",
    "    out_rows = []\n",
    "    for smi in smiles_series.astype(str).tolist():\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol is None:\n",
    "            out_rows.append({n: on_fail for n in desc_names})\n",
    "            continue\n",
    "        row = {}\n",
    "        for n in desc_names:\n",
    "            try:\n",
    "                row[n] = float(avail[n](mol))\n",
    "            except Exception:\n",
    "                row[n] = on_fail\n",
    "        out_rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(out_rows, index=smiles_series.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'types.GenericAlias' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 43\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[39mreturn\u001b[39;00m keep\n\u001b[1;32m     37\u001b[0m \u001b[39m# ===== 3) 组装某个目标的训练/测试特征矩阵（可拼接你的字符级特征列） =====\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mbuild_Xy_for_target\u001b[39m(\n\u001b[1;32m     39\u001b[0m     df: pd\u001b[39m.\u001b[39mDataFrame,               \u001b[39m# 含 SMILES 与各目标的总表（已去掉无效 SMILES）\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     desc_df: pd\u001b[39m.\u001b[39mDataFrame,          \u001b[39m# 与 df 行对齐的 RDKit 描述符全表\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     target: \u001b[39mstr\u001b[39m,                    \u001b[39m# 'Tg' / 'FFV' / ...\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     filters: \u001b[39mdict\u001b[39m,                  \u001b[39m# 你给的 filters 字典\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     extra_feat_cols: \u001b[39mlist\u001b[39;49m[\u001b[39mstr\u001b[39;49m] \u001b[39m|\u001b[39;49m \u001b[39mNone\u001b[39;49;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,   \u001b[39m# 可选：你的字符级/其它特征列名\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     smiles_col: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSMILES\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     45\u001b[0m ):\n\u001b[1;32m     46\u001b[0m     \u001b[39m# 选择该目标要用的 RDKit 列\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     rd_cols \u001b[39m=\u001b[39m map_and_select(filters[target], desc_df)\n\u001b[1;32m     48\u001b[0m     parts \u001b[39m=\u001b[39m [desc_df[rd_cols]]\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'types.GenericAlias' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# ===== 1) 计算 RDKit 全量官方描述符（不含手工字段） =====\n",
    "def compute_all_rdkit_descriptors(smiles_series: pd.Series) -> pd.DataFrame:\n",
    "    names, funcs = zip(*Descriptors.descList)          # 官方名称 + 计算函数\n",
    "    rows = []\n",
    "    for s in smiles_series.astype(str):\n",
    "        mol = Chem.MolFromSmiles(s)\n",
    "        if mol is None:\n",
    "            rows.append([np.nan]*len(funcs))\n",
    "            continue\n",
    "        vals = []\n",
    "        for f in funcs:\n",
    "            try:\n",
    "                vals.append(float(f(mol)))\n",
    "            except Exception:\n",
    "                vals.append(np.nan)\n",
    "        rows.append(vals)\n",
    "    return pd.DataFrame(rows, columns=list(names), index=smiles_series.index)\n",
    "\n",
    "# ===== 2) 别名映射，确保 filters 里的名字能对上 RDKit 列 =====\n",
    "ALIAS = {\n",
    "    'LogP': 'MolLogP',\n",
    "    'RotatableBonds': 'NumRotatableBonds',\n",
    "    'HBA': 'NumHAcceptors',\n",
    "    'HBD': 'NumHDonors',\n",
    "    'NumHetero': 'NumHeteroatoms',\n",
    "    'RingCount': 'NumRings',        # 你 filters 里是 RingCount，RDKit 通常叫 NumRings\n",
    "}\n",
    "\n",
    "def map_and_select(filter_names: list[str], desc_df: pd.DataFrame) -> list[str]:\n",
    "    mapped = [ALIAS.get(n, n) for n in filter_names]\n",
    "    keep = [n for n in mapped if n in desc_df.columns]\n",
    "    missing = [n for n in mapped if n not in desc_df.columns]\n",
    "    if missing:\n",
    "        print(f\"[warn] dropped (not in RDKit): {missing[:12]}{' ...' if len(missing)>12 else ''}\")\n",
    "    return keep\n",
    "\n",
    "# ===== 3) 组装某个目标的训练/测试特征矩阵（可拼接你的字符级特征列） =====\n",
    "def build_Xy_for_target(\n",
    "    df: pd.DataFrame,               # 含 SMILES 与各目标的总表（已去掉无效 SMILES）\n",
    "    desc_df: pd.DataFrame,          # 与 df 行对齐的 RDKit 描述符全表\n",
    "    target: str,                    # 'Tg' / 'FFV' / ...\n",
    "    filters: dict,                  # 你给的 filters 字典\n",
    "    extra_feat_cols: list[str] | None = None,   # 可选：你的字符级/其它特征列名\n",
    "    smiles_col: str = 'SMILES'\n",
    "):\n",
    "    # 选择该目标要用的 RDKit 列\n",
    "    rd_cols = map_and_select(filters[target], desc_df)\n",
    "    parts = [desc_df[rd_cols]]\n",
    "    if extra_feat_cols:\n",
    "        # 只取数值列 & 现存列\n",
    "        extra = [c for c in extra_feat_cols if c in df.columns and pd.api.types.is_numeric_dtype(df[c])]\n",
    "        if extra:\n",
    "            parts.append(df[extra])\n",
    "    X_full = pd.concat(parts, axis=1)\n",
    "\n",
    "    # 仅保留该目标有标签的行\n",
    "    rows = df[target].notna()\n",
    "    X = X_full.loc[rows].copy()\n",
    "    y = df.loc[rows, target].astype(float).copy()\n",
    "    return X, y, list(X.columns)\n",
    "\n",
    "# ===== 4) 简单的中位数填充（只填特征，不动 target） =====\n",
    "def median_impute(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    for c in X.columns:\n",
    "        if X[c].isna().any():\n",
    "            X[c] = X[c].fillna(X[c].median())\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# 先一次性算好 RDKit 全量描述符\u001b[39;00m\n\u001b[1;32m      2\u001b[0m desc_train \u001b[39m=\u001b[39m compute_all_rdkit_descriptors(train_clean[\u001b[39m'\u001b[39m\u001b[39mSMILES\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m desc_test  \u001b[39m=\u001b[39m compute_all_rdkit_descriptors(test_clean[\u001b[39m'\u001b[39m\u001b[39mSMILES\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_clean' is not defined"
     ]
    }
   ],
   "source": [
    "# 先一次性算好 RDKit 全量描述符\n",
    "desc_train = compute_all_rdkit_descriptors(train_clean['SMILES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tg, y_tg, tg_cols = build_Xy_for_target(\n",
    "    df=train_clean,\n",
    "    desc_df=desc_train,\n",
    "    target='Tg',\n",
    "    filters=filters,\n",
    "    extra_feat_cols=char_feat_cols\n",
    ")\n",
    "X_tg = median_impute(X_tg)                 # 只填特征\n",
    "print(\"Tg features:\", len(tg_cols), X_tg.shape, y_tg.shape)\n",
    "\n",
    "# 测试集用相同列\n",
    "# 注意：desc_test 只有描述符，没有 target；extra 特征直接从 test_clean 取\n",
    "X_tg_test = pd.concat(\n",
    "    [\n",
    "        desc_test[[c for c in tg_cols if c in desc_test.columns]],\n",
    "        test_clean[[c for c in tg_cols if c in test_clean.columns]]  # 这行通常为空；仅当 tg_cols 里包含了你的字符列名时才会取到\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "# 对齐训练列次序（有的列在测试缺失，用 NaN 占位后再中位数填充）\n",
    "X_tg_test = X_tg_test.reindex(columns=tg_cols)\n",
    "X_tg_test = median_impute(X_tg_test)\n",
    "print(\"Tg test matrix:\", X_tg_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86715117325bb669dcee076931360a6797c084140656b2fef4bf943c7ad69ff7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
