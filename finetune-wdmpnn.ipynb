{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c484d80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, types, math, torch, torch.nn as nn\n",
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_preparation import PolymerDataset, TARGETS, make_smile_canonical, get_data_paths, add_extra_data, load_polymer_dataset\n",
    "from model import WDMPNNModel, MultiTaskHead\n",
    "from train import fit  # 直接复用你已有的 fit()\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 训练超参\n",
    "HIDDEN_DIM = 128\n",
    "MLP_HIDDEN = [256, 128]\n",
    "NUM_EDGE_LAYERS = 4\n",
    "DROPOUT = 0.001\n",
    "LR = 5e-4\n",
    "WARMUP_EPOCHS = 3\n",
    "FINETUNE_EPOCHS = 20\n",
    "\n",
    "# 手动填写（按你 QM9 预训练时的输入维度）\n",
    "QM9_NODE_DIM = 11  # e.g., 11（举例，按你实际为准）\n",
    "QM9_EDGE_DIM = 4  # e.g., 4\n",
    "# 选择一个预训练最优权重\n",
    "CKPT_PATH = \"checkpoints/trial_29_best.pt\"  # 换成你想用的那个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e7ac70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv: kaggle\\input\\neurips-open-polymer-prediction-2025\\train.csv  [OK]\n",
      "test_csv: kaggle\\input\\neurips-open-polymer-prediction-2025\\test.csv  [OK]\n",
      "sample_submission: kaggle\\input\\neurips-open-polymer-prediction-2025\\sample_submission.csv  [OK]\n",
      "tc_data: kaggle\\input\\tc-smiles\\Tc_SMILES.csv  [OK]\n",
      "tg_jcim_data: kaggle\\input\\smiles-extra-data\\JCIM_sup_bigsmiles.csv  [OK]\n",
      "tg_excel_data: kaggle\\input\\smiles-extra-data\\data_tg3.xlsx  [OK]\n",
      "density_data: kaggle\\input\\smiles-extra-data\\data_dnst1.xlsx  [OK]\n",
      "ffv_data: kaggle\\input\\neurips-open-polymer-prediction-2025\\train_supplement\\dataset4.csv  [OK]\n",
      "dataset1: kaggle\\input\\neurips-open-polymer-prediction-2025\\train_supplement\\dataset1.csv  [OK]\n",
      "dataset2: kaggle\\input\\neurips-open-polymer-prediction-2025\\train_supplement\\dataset2.csv  [OK]\n",
      "dataset3: kaggle\\input\\neurips-open-polymer-prediction-2025\\train_supplement\\dataset3.csv  [OK]\n",
      "Train/Test/Sub: (7973, 7) (3, 2) (3, 6)\n"
     ]
    }
   ],
   "source": [
    "P = get_data_paths()\n",
    "for k, v in P.items():\n",
    "    print(f\"{k}: {v}  {'[OK]' if v.exists() else '[MISSING]'}\")\n",
    "\n",
    "# 核心官方数据\n",
    "train = pd.read_csv(P['train_csv'])\n",
    "test  = pd.read_csv(P['test_csv'])\n",
    "sub   = pd.read_csv(P['sample_submission'])\n",
    "print(\"Train/Test/Sub:\", train.shape, test.shape, sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1420ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Working on tc_data (target=Tc)\n",
      "  重复 SMILES: 737, 其中 2 条数值不同 (mean diff=0.000)\n",
      "  [Tc] 增强: +130 条新样本, 填补 130 条缺失\n",
      "[INFO] Working on tg_jcim (target=Tg)\n",
      "  重复 SMILES: 7, 其中 0 条数值不同 (mean diff=0.000)\n",
      "  [Tg] 增强: +655 条新样本, 填补 655 条缺失\n",
      "[INFO] Working on tg_excel_K_to_C (target=Tg)\n",
      "  [Tg] 增强: +499 条新样本, 填补 499 条缺失\n",
      "[INFO] Working on dataset3 (target=Tg)\n",
      "  [Tg] 增强: +46 条新样本, 填补 46 条缺失\n",
      "[INFO] Working on density_extra_minus_0p118 (target=Density)\n",
      "  重复 SMILES: 4, 其中 2 条数值不同 (mean diff=0.055)\n",
      "  [Density] 增强: +782 条新样本, 填补 784 条缺失\n",
      "[INFO] Working on dataset1 (target=Tc)\n",
      "  重复 SMILES: 867, 其中 2 条数值不同 (mean diff=0.000)\n",
      "  [Tc] 增强: +0 条新样本, 填补 0 条缺失\n",
      "[INFO] Working on ffv_dataset4 (target=FFV)\n",
      "  重复 SMILES: 37, 其中 0 条数值不同 (mean diff=nan)\n",
      "  [FFV] 增强: +825 条新样本, 填补 862 条缺失\n",
      "train: (10910, 7)\n",
      "targets non-null: {'Tg': 1711, 'FFV': 7892, 'Tc': 867, 'Density': 1397, 'Rg': 614}\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(P['tc_data']):       \n",
    "    train = add_extra_data(\n",
    "        train,\n",
    "        pd.read_csv(P['tc_data']).rename(columns={'TC_mean':'Tc'}),\n",
    "        target='Tc',\n",
    "        source_name='tc_data'\n",
    "    )\n",
    "\n",
    "if os.path.exists(P['tg_jcim_data']):  \n",
    "    train = add_extra_data(\n",
    "        train,\n",
    "        pd.read_csv(P['tg_jcim_data'], usecols=['SMILES','Tg (C)']).rename(columns={'Tg (C)':'Tg'}),\n",
    "        target='Tg',\n",
    "        source_name='tg_jcim'\n",
    "    )\n",
    "\n",
    "if os.path.exists(P['tg_excel_data']):\n",
    "    train = add_extra_data(\n",
    "        train,\n",
    "        pd.read_excel(P['tg_excel_data']).rename(columns={'Tg [K]':'Tg'}).assign(Tg=lambda df: df['Tg'] - 273.15),\n",
    "        target='Tg',\n",
    "        source_name='tg_excel_K_to_C'\n",
    "    )\n",
    "\n",
    "if os.path.exists(P['dataset3']):\n",
    "    train = add_extra_data(\n",
    "        train,\n",
    "        pd.read_csv(P['dataset3']),  # 已是列名 SMILES, Tg\n",
    "        target='Tg',\n",
    "        source_name='dataset3'\n",
    "    )\n",
    "\n",
    "if os.path.exists(P['density_data']):\n",
    "    train = add_extra_data(\n",
    "        train,\n",
    "        pd.read_excel(P['density_data'])\n",
    "          .rename(columns={'density(g/cm3)':'Density'})\n",
    "          .assign(Density=lambda df: pd.to_numeric(df['Density'], errors='coerce') - 0.118),\n",
    "        target='Density',\n",
    "        source_name='density_extra_minus_0p118'\n",
    "    )\n",
    "\n",
    "if os.path.exists(P['dataset1']):\n",
    "    train = add_extra_data(\n",
    "        train,\n",
    "        pd.read_csv(P['dataset1']).rename(columns={'TC_mean':'Tc'}),\n",
    "        target='Tc',\n",
    "        source_name='dataset1'\n",
    "    )\n",
    "\n",
    "if os.path.exists(P['ffv_data']):\n",
    "    train = add_extra_data(\n",
    "        train,\n",
    "        pd.read_csv(P['ffv_data']).rename(columns={'FFV':'FFV'}),\n",
    "        target='FFV',\n",
    "        source_name='ffv_dataset4'\n",
    "    )\n",
    "\n",
    "print(f\"train: {train.shape}\")\n",
    "print(\"targets non-null:\", {t: int(train[t].notna().sum()) for t in TARGETS if t in train.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23464f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'uid'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m val_loader   \u001b[38;5;241m=\u001b[39m DataLoader(val_set,   batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 聚合物原始维度（供 Adapter 使用）\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m b0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m POLY_NODE_DIM \u001b[38;5;241m=\u001b[39m b0\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m POLY_EDGE_DIM \u001b[38;5;241m=\u001b[39m b0\u001b[38;5;241m.\u001b[39medge_attr\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\neurips\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    740\u001b[0m ):\n",
      "File \u001b[1;32md:\\anaconda\\envs\\neurips\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\neurips\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\neurips\\lib\\site-packages\\torch_geometric\\loader\\dataloader.py:27\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     25\u001b[0m elem \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, BaseData):\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_data_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\neurips\\lib\\site-packages\\torch_geometric\\data\\batch.py:97\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[1;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_data_list\u001b[39m(\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m     exclude_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m    list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m     batch, slice_dict, inc_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincrement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_num_graphs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_list)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_slice_dict \u001b[38;5;241m=\u001b[39m slice_dict  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\neurips\\lib\\site-packages\\torch_geometric\\data\\collate.py:95\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m exclude_keys:  \u001b[38;5;66;03m# Do not include top-level attribute.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m values \u001b[38;5;241m=\u001b[39m [store[attr] \u001b[38;5;28;01mfor\u001b[39;00m store \u001b[38;5;129;01min\u001b[39;00m stores]\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# The `num_nodes` attribute needs special treatment, as we need to\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# sum their values up instead of merging them to a list:\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_nodes\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\neurips\\lib\\site-packages\\torch_geometric\\data\\collate.py:95\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m exclude_keys:  \u001b[38;5;66;03m# Do not include top-level attribute.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m values \u001b[38;5;241m=\u001b[39m [\u001b[43mstore\u001b[49m\u001b[43m[\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m store \u001b[38;5;129;01min\u001b[39;00m stores]\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# The `num_nodes` attribute needs special treatment, as we need to\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# sum their values up instead of merging them to a list:\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_nodes\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\neurips\\lib\\site-packages\\torch_geometric\\data\\storage.py:118\u001b[0m, in \u001b[0;36mBaseStorage.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'uid'"
     ]
    }
   ],
   "source": [
    "df = train.copy()\n",
    "df[\"SMILES\"] = df[\"SMILES\"].apply(make_smile_canonical)\n",
    "df = df[df[\"SMILES\"].notnull()].reset_index(drop=True)\n",
    "\n",
    "\n",
    "poly_dataset_full = PolymerDataset(df, transform=None)\n",
    "\n",
    "# 随机切分 train/val\n",
    "total = len(poly_dataset_full)\n",
    "val_sz = max(1, int(0.1 * total))\n",
    "train_sz = total - val_sz\n",
    "g = torch.Generator().manual_seed(42)\n",
    "train_set, val_set = torch.utils.data.random_split(poly_dataset_full, [train_sz, val_sz], generator=g)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=32, shuffle=False)\n",
    "\n",
    "# 聚合物原始维度（供 Adapter 使用）\n",
    "b0 = next(iter(train_loader))\n",
    "POLY_NODE_DIM = b0.x.size(-1)\n",
    "POLY_EDGE_DIM = b0.edge_attr.size(-1)\n",
    "print(\"POLY dims:\", POLY_NODE_DIM, POLY_EDGE_DIM, \"| train/val:\", len(train_set), len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f6893a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAdapter(nn.Module):\n",
    "    \"\"\"in_dim -> out_dim 的轻量投影（可选瓶颈 + LayerNorm + 小门控）\"\"\"\n",
    "    def __init__(self, in_dim, out_dim, bottleneck=64, p_drop=0.1, use_gate=True, use_ln=True):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(in_dim) if use_ln else nn.Identity()\n",
    "        if bottleneck and bottleneck < min(in_dim, out_dim):\n",
    "            self.proj = nn.Sequential(\n",
    "                nn.Linear(in_dim, bottleneck), nn.GELU(), nn.Dropout(p_drop),\n",
    "                nn.Linear(bottleneck, out_dim),\n",
    "            )\n",
    "        else:\n",
    "            self.proj = nn.Linear(in_dim, out_dim)\n",
    "        self.gate = nn.Parameter(torch.tensor(0.01)) if use_gate else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        y = self.proj(x)\n",
    "        return y * self.gate if self.gate is not None else y\n",
    "\n",
    "def attach_adapters(model, poly_node_dim, poly_edge_dim, qm9_node_dim, qm9_edge_dim):\n",
    "    \"\"\"在 encoder 前挂 adapter，并用 monkey-patch 覆盖 forward。\"\"\"\n",
    "    model.node_adapter = LinearAdapter(poly_node_dim, qm9_node_dim, bottleneck=64, p_drop=0.1)\n",
    "    model.edge_adapter = LinearAdapter(poly_edge_dim, qm9_edge_dim, bottleneck=32, p_drop=0.1)\n",
    "\n",
    "    def forward_with_adapter(self, data):\n",
    "        edge_weight = getattr(data, \"edge_weight\", None)\n",
    "        x2 = self.node_adapter(data.x)\n",
    "        e2 = self.edge_adapter(data.edge_attr)\n",
    "        g = self.encoder(x2, data.edge_index, e2, data.batch, edge_weight=edge_weight)\n",
    "        return self.head(g)\n",
    "\n",
    "    model.forward = types.MethodType(forward_with_adapter, model)\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_task_stats_from_dataset(dataset, tasks):\n",
    "    \"\"\"wMAE 所需 n_i / r_i（从 dataset 内的 y & mask 统计）\"\"\"\n",
    "    n = {t: 0 for t in tasks}\n",
    "    lo = {t: math.inf for t in tasks}\n",
    "    hi = {t: -math.inf for t in tasks}\n",
    "    for d in getattr(dataset, \"data_list\", dataset):\n",
    "        y = d.y.cpu()\n",
    "        m = d.mask.cpu()\n",
    "        for i, t in enumerate(tasks):\n",
    "            if m[i] > 0.5:\n",
    "                n[t] += 1\n",
    "                v = float(y[i])\n",
    "                lo[t] = min(lo[t], v)\n",
    "                hi[t] = max(hi[t], v)\n",
    "    r = {t: (0.0 if not math.isfinite(lo[t]) or not math.isfinite(hi[t]) else (hi[t]-lo[t])) for t in tasks}\n",
    "    return n, r\n",
    "\n",
    "def load_trunk_only(model, ckpt_path, device=DEVICE):\n",
    "    \"\"\"从 trial_*_best.pt 里只加载 encoder.*（丢弃 head.*）\"\"\"\n",
    "    sd_full = torch.load(ckpt_path, map_location=device)\n",
    "    state = sd_full.get(\"model_state\", sd_full)  # 兼容纯 state_dict\n",
    "    trunk = {k: v for k, v in state.items() if k.startswith(\"encoder.\")}\n",
    "    missing, unexpected = model.load_state_dict(trunk, strict=False)\n",
    "    print(f\"Loaded trunk: {len(trunk)} tensors | missing={len(missing)} | unexpected={len(unexpected)}\")\n",
    "\n",
    "def freeze_encoder_only(model, freeze=True):\n",
    "    \"\"\"warmup 期：只训 adapter+head；finetune 期：全解冻。\"\"\"\n",
    "    for n, p in model.named_parameters():\n",
    "        is_adapter = n.startswith((\"node_adapter\", \"edge_adapter\"))\n",
    "        is_head = n.startswith(\"head.\")\n",
    "        if freeze:\n",
    "            if not (is_adapter or is_head):\n",
    "                p.requires_grad_(False)\n",
    "        else:\n",
    "            p.requires_grad_(True)\n",
    "\n",
    "def build_optimizer_grouped(model, base_lr=1e-3):\n",
    "    \"\"\"分组学习率：adapter > head > trunk。\"\"\"\n",
    "    adapter_params, head_params, trunk_params = [], [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad: continue\n",
    "        if n.startswith((\"node_adapter\", \"edge_adapter\")):\n",
    "            adapter_params.append(p)\n",
    "        elif n.startswith(\"head.\"):\n",
    "            head_params.append(p)\n",
    "        else:\n",
    "            trunk_params.append(p)\n",
    "    return torch.optim.Adam([\n",
    "        {\"params\": adapter_params, \"lr\": base_lr * 2.0},\n",
    "        {\"params\": head_params,    \"lr\": base_lr * 1.0},\n",
    "        {\"params\": trunk_params,   \"lr\": base_lr * 0.2},\n",
    "    ], lr=base_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46e32f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trunk: 10 tensors | missing=40 | unexpected=0\n",
      "Ready. Hidden = 128 | params trainable = 367382\n"
     ]
    }
   ],
   "source": [
    "from data_preparation import TARGETS  # 比赛任务\n",
    "\n",
    "# 1) 用 QM9 维度构建骨架（tasks=TARGETS）\n",
    "model = WDMPNNModel(\n",
    "    node_feat_dim=QM9_NODE_DIM,\n",
    "    edge_feat_dim=QM9_EDGE_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    mlp_hidden=MLP_HIDDEN,\n",
    "    tasks=TARGETS,\n",
    "    num_edge_layers=NUM_EDGE_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)\n",
    "\n",
    "# 2) 前置适配器\n",
    "attach_adapters(model, POLY_NODE_DIM, POLY_EDGE_DIM, QM9_NODE_DIM, QM9_EDGE_DIM)\n",
    "\n",
    "# 3) 任务权重（wMAE）\n",
    "n_dict, r_dict = compute_task_stats_from_dataset(poly_dataset_full, TARGETS)\n",
    "model.set_task_stats(n_dict, r_dict)\n",
    "\n",
    "# 4) 只加载 encoder 主干权重\n",
    "load_trunk_only(model, CKPT_PATH, device=DEVICE)\n",
    "\n",
    "print(\"Ready. Hidden =\", model.encoder.hidden_dim, \"| params trainable =\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "051e9e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Device: cpu | Params: 0.37M | Tasks: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/3] Train 0.072, Val 0.066 | Tg:82.783 | FFV:0.027 | Tc:0.067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/3] Train 0.062, Val 0.053 | Tg:82.388 | FFV:0.020 | Tc:0.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/3] Train 0.055, Val 0.051 | Tg:80.201 | FFV:0.019 | Tc:0.042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "freeze_encoder_only(model, freeze=True)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\":[p for n,p in model.named_parameters() if n.startswith((\"node_adapter\",\"edge_adapter\"))], \"lr\": LR*5},\n",
    "    {\"params\":[p for n,p in model.named_parameters() if n.startswith(\"head.\")], \"lr\": LR*2},\n",
    "], lr=LR)\n",
    "\n",
    "_ = fit(model, train_loader, val_loader, optimizer, DEVICE, TARGETS,\n",
    "        epochs=WARMUP_EPOCHS, use_mask=True, run_wandb=False,\n",
    "        project=\"polymer-finetune-adapter-warmup\",\n",
    "        ckpt_path=\"checkpoints/poly_adapter_warmup_best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "707cd010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Device: cpu | Params: 0.37M | Tasks: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/20] Train 0.052, Val 0.053 | Tg:74.267 | FFV:0.018 | Tc:0.039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/20] Train 0.050, Val 0.047 | Tg:69.485 | FFV:0.017 | Tc:0.037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m freeze_encoder_only(model, freeze\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m build_optimizer_grouped(model, base_lr\u001b[38;5;241m=\u001b[39mLR)\n\u001b[1;32m----> 4\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTARGETS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFINETUNE_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m              \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpolymer-finetune-adapter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m              \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoints/poly_adapter_finetune_best.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Teyer\\kaggle-neurips-0907\\train.py:103\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, train_loader, val_loader, optimizer, device, tasks, epochs, use_mask, patience, run_wandb, project, print_all_tasks, ckpt_path)\u001b[0m\n\u001b[0;32m    100\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(save_path) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 103\u001b[0m     tr_loss, tr_mae \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     val_loss, val_mae \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, device, tasks, use_mask, epoch, epochs)\n\u001b[0;32m    106\u001b[0m     history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(tr_loss)\n",
      "File \u001b[1;32mc:\\Users\\Teyer\\kaggle-neurips-0907\\train.py:27\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, loader, optimizer, device, tasks, use_mask, epoch, epochs)\u001b[0m\n\u001b[0;32m     25\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 27\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m targets, mask \u001b[38;5;241m=\u001b[39m _targets_masks_from_batch(batch, tasks, use_mask\u001b[38;5;241m=\u001b[39muse_mask)\n\u001b[0;32m     31\u001b[0m loss, mae_dict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompute_wmae_loss(outputs, targets, mask\u001b[38;5;241m=\u001b[39mmask)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\neurips\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\neurips\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[5], line 27\u001b[0m, in \u001b[0;36mattach_adapters.<locals>.forward_with_adapter\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward_with_adapter\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m     26\u001b[0m     edge_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 27\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     e2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_adapter(data\u001b[38;5;241m.\u001b[39medge_attr)\n\u001b[0;32m     29\u001b[0m     g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x2, data\u001b[38;5;241m.\u001b[39medge_index, e2, data\u001b[38;5;241m.\u001b[39mbatch, edge_weight\u001b[38;5;241m=\u001b[39medge_weight)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\neurips\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\neurips\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m, in \u001b[0;36mLinearAdapter.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m---> 17\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m y\n",
      "File \u001b[1;32md:\\anaconda\\envs\\neurips\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\neurips\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\anaconda\\envs\\neurips\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "freeze_encoder_only(model, freeze=False)\n",
    "optimizer = build_optimizer_grouped(model, base_lr=LR)\n",
    "\n",
    "history = fit(model, train_loader, val_loader, optimizer, DEVICE, TARGETS,\n",
    "              epochs=FINETUNE_EPOCHS, use_mask=True, run_wandb=False,\n",
    "              project=\"polymer-finetune-adapter\",\n",
    "              ckpt_path=\"checkpoints/poly_adapter_finetune_best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65a43d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tg': array([ 31.952337, -17.538939,  85.358826,  73.86589 ,  57.80554 ],\n",
       "       dtype=float32),\n",
       " 'FFV': array([0.3515721 , 0.3814314 , 0.36397016, 0.35942668, 0.35875398],\n",
       "       dtype=float32),\n",
       " 'Tc': array([0.25661173, 0.38523132, 0.23553681, 0.20132993, 0.21671148],\n",
       "       dtype=float32),\n",
       " 'Density': array([1.0350479 , 0.89208084, 1.0976292 , 1.1937238 , 1.0376378 ],\n",
       "       dtype=float32),\n",
       " 'Rg': array([18.588612, 19.274672, 19.26153 , 18.443525, 18.358446],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"checkpoints/poly_adapter_finetune_best.pt\", map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_batch(model, batch):\n",
    "    out = model(batch.to(DEVICE))\n",
    "    return {t: out[t].cpu().numpy() for t in model.tasks}\n",
    "\n",
    "batch = next(iter(val_loader))\n",
    "pred = predict_batch(model, batch)\n",
    "{k: v[:5] for k, v in pred.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf87655e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_polymer_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_loader, test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_polymer_dataset\u001b[49m(P[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_csv\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 可选：看看有没有被丢弃的样本\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(test_dataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropped\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_polymer_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "test_loader, test_dataset = load_polymer_dataset(P[\"test_csv\"], batch_size=64, shuffle=False)\n",
    "\n",
    "# 可选：看看有没有被丢弃的样本\n",
    "if getattr(test_dataset, \"dropped\", None):\n",
    "    print(\"Dropped samples:\", len(test_dataset.dropped), test_dataset.dropped[:3])\n",
    "\n",
    "# 2) 载入你已经训练好的“带 adapter”的模型\n",
    "model.load_state_dict(torch.load(\"checkpoints/poly_adapter_finetune_best.pt\", map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# 3) 逐批预测（同一个 forward）\n",
    "import pandas as pd\n",
    "@torch.no_grad()\n",
    "def predict_loader(model, loader):\n",
    "    rows = []\n",
    "    for batch in loader:\n",
    "        batch = batch.to(DEVICE)\n",
    "        out = model(batch)  # dict{task: Tensor[B]}\n",
    "        # 如果 test 有 id，则在构图时已保存到 data.uid\n",
    "        uids = batch.uid.view(-1).tolist() if hasattr(batch, \"uid\") else list(range(len(rows), len(rows) + out[TARGETS[0]].shape[0]))\n",
    "        for i, uid in enumerate(uids):\n",
    "            row = {\"id\": int(uid)}\n",
    "            for t in model.tasks:\n",
    "                row[t] = float(out[t][i].detach().cpu().item())\n",
    "            rows.append(row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "pred_df = predict_loader(model, test_loader)\n",
    "\n",
    "# 4) 与原 test.csv 对齐并保存\n",
    "test_raw = pd.read_csv(P[\"test_csv\"])[[\"id\"]]\n",
    "submission = test_raw.merge(pred_df, on=\"id\", how=\"left\")\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Saved submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cd2025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurips",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
